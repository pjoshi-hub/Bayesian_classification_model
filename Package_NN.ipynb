{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_wise_results(dict_numtest_samples,y_test,arr_pred_test,aleatoric_uncertainty,epistemic_uncertainty):\n",
    "    dict_result = {}\n",
    "    temp_prev = 0\n",
    "    list_accuracy = []\n",
    "    for i in range(31):\n",
    "        temp = dict_numtest_samples[i]\n",
    "        sliced_y_pred = arr_pred_test[temp_prev:temp_prev+temp]\n",
    "        sliced_y_test = y_test[temp_prev:temp_prev+temp]\n",
    "        al_uncertainty_test = aleatoric_uncertainty[temp_prev:temp_prev+temp]\n",
    "        ep_uncertainty_test = epistemic_uncertainty[temp_prev:temp_prev+temp]\n",
    "        dict_result[i] = [sliced_y_test,sliced_y_pred,al_uncertainty_test,ep_uncertainty_test]\n",
    "        temp_prev = temp_prev+temp\n",
    "    return dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def placeholders(n_x,n_y):\n",
    "    X = tf.placeholder(tf.float32,shape=[n_x,None],name='X')\n",
    "    Y = tf.placeholder(tf.float32,shape=[n_y,None],name='Y')\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,layers_dims,param_normal, keep_prob=1.0):\n",
    "    \n",
    "    def sample_epsilons(param_normal):\n",
    "        epsilons_W = []\n",
    "        epsilons_b = []\n",
    "        for i in range(len(layers_dims)-1):\n",
    "            epsilons_W.append(tf.random_normal(shape=tf.shape(param_normal[\"mu_W\"+str(i+1)]), mean=0., stddev=1.0))\n",
    "            epsilons_b.append(tf.random_normal(shape=tf.shape(param_normal[\"mu_b\"+str(i+1)]), mean=0., stddev=1.0))\n",
    "        return epsilons_W,epsilons_b\n",
    "\n",
    "    def transform_rhos(layers_dims,param_normal):\n",
    "        for i in range(len(layers_dims)-1):\n",
    "            param_normal[\"rho_W\"+str(i+1)] = softplus(param_normal[\"rho_W\"+str(i+1)])\n",
    "            param_normal[\"rho_b\"+str(i+1)] = softplus(param_normal[\"rho_b\"+str(i+1)])\n",
    "        return param_normal\n",
    "\n",
    "    def make_gaussian_samples(param_normal,layers_dims,epsilons_W,epsilons_b):\n",
    "        samples_W = []\n",
    "        samples_b = []\n",
    "        for i in range(len(layers_dims)-1):\n",
    "            samples_W.append(tf.add(param_normal[\"mu_W\"+str(i+1)],tf.multiply( param_normal[\"rho_W\"+str(i+1)] , epsilons_W[i])))\n",
    "            samples_b.append(tf.add(param_normal[\"mu_b\"+str(i+1)] ,tf.multiply( param_normal[\"rho_b\"+str(i+1)] , epsilons_b[i])))\n",
    "        return samples_W, samples_b\n",
    "\n",
    "    epsilons_W,epsilons_b = sample_epsilons(param_normal)\n",
    "    param_normal = transform_rhos(layers_dims,param_normal)\n",
    "    samples_W, samples_b =  make_gaussian_samples(param_normal,layers_dims,epsilons_W,epsilons_b)\n",
    "    \n",
    "    store = {}\n",
    "    store['A0'] = X\n",
    "    for l in range(len(layers_dims)-1):\n",
    "        store[\"Z\"+str(l+1)] = tf.add(tf.matmul(samples_W[l],store[\"A\"+str(l)]),samples_b[l])\n",
    "        if (l == len(layers_dims) - 2):\n",
    "            return store[\"Z\"+str(l+1)],samples_W,samples_b\n",
    "        #store[\"Z\"+str(l+1)] = tf.layers.batch_normalization(store[\"Z\"+str(l+1)],axis=0)\n",
    "        store[\"A\"+str(l+1)] = tf.nn.sigmoid(store[\"Z\"+str(l+1)])\n",
    "        store[\"A\"+str(l+1)] = tf.nn.dropout(store[\"A\"+str(l+1)], keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(layers_dims):\n",
    "    param_normal = {}\n",
    "    for l in range(len(layers_dims)-1):\n",
    "        param_normal[\"mu_W\"+str(l+1)] = tf.get_variable('mu_W'+str(l+1),[layers_dims[l+1],layers_dims[l]],initializer =  tf.random_normal_initializer(mean = 0.0,stddev = 0.1))\n",
    "        param_normal[\"rho_W\"+str(l+1)] = -15.5 + tf.get_variable(\"rho_W\"+str(l+1),[layers_dims[l+1],layers_dims[l]],initializer = tf.zeros_initializer())\n",
    "        param_normal[\"mu_b\"+str(l+1)] = tf.get_variable('mu_b'+str(l+1),[layers_dims[l+1],1],initializer =  tf.random_normal_initializer(mean = 0.0,stddev = 0.1))\n",
    "        param_normal[\"rho_b\"+str(l+1)] =  -16.5 + tf.get_variable(\"rho_b\"+str(l+1),[layers_dims[l+1],1],initializer = tf.zeros_initializer())\n",
    "    return param_normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return tf.log(1.0 + tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gaussian(x, mu,sigma):\n",
    "    return -0.5 * tf.log(2.0 * tf.constant(math.pi)) - tf.log(sigma) - tf.truediv(tf.multiply((x-mu),(x-mu)), (2.0 * tf.multiply(sigma,sigma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(x):\n",
    "    mean_prior = tf.constant(0.0)\n",
    "    sigma_prior = tf.constant(1.0)\n",
    "    return tf.reduce_sum(log_gaussian(x,mean_prior,sigma_prior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x,mu,sigma):\n",
    "    scaling = tf.truediv(1.0,tf.sqrt(2.0 * tf.constant(math.pi) * tf.multiply(sigma,sigma)))\n",
    "    bell = tf.exp(-1.0 * tf.truediv(tf.multiply((x-mu),(x-mu)), (2.0 * tf.multiply(sigma,sigma))))\n",
    "    return tf.multiply(scaling,bell)\n",
    "\n",
    "def scale_mixture_prior(x):\n",
    "    sigma_p1 = tf.constant(0.2)\n",
    "    sigma_p2 = tf.constant(0.8)\n",
    "    pi = 0.15\n",
    "    first_gaussian = tf.constant(pi) * gaussian(x,0.0,sigma_p1)\n",
    "    second_gaussian = (1.0-tf.constant(pi)) * gaussian(x,0.0,sigma_p2)\n",
    "    return tf.reduce_sum(tf.log(first_gaussian+second_gaussian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax_likelihood(ZL, y):\n",
    "    return  -1 * tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=tf.transpose(y),logits=tf.transpose(ZL),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(layers_dims,samples_W,samples_b,param_normal,ZL,label_one_hot):\n",
    "    log_likelihood_sum = log_softmax_likelihood(ZL, label_one_hot)\n",
    "    log_prior_list = []\n",
    "    log_var_posterior_list = []\n",
    "    for i in range(len(layers_dims)-1):\n",
    "        log_prior_list.append(prior(samples_W[i]))\n",
    "        log_prior_list.append(prior(samples_b[i]))\n",
    "        log_var_posterior_list.append(tf.reduce_sum(log_gaussian(samples_W[i],param_normal[\"mu_W\"+str(i+1)],param_normal[\"rho_W\"+str(i+1)])))\n",
    "        log_var_posterior_list.append( tf.reduce_sum(log_gaussian(samples_b[i],param_normal[\"mu_b\"+str(i+1)],param_normal[\"rho_b\"+str(i+1)])))\n",
    "    log_prior_sum = sum(log_prior_list)\n",
    "    log_var_posterior_sum = sum(log_var_posterior_list)\n",
    "    return 1/(X_train.shape[1]) * (log_var_posterior_sum - log_prior_sum -  log_likelihood_sum)#log_likelihood_sum #(log_var_posterior_sum - log_prior_sum -  log_likelihood_sum)   #((log_var_posterior_sum - log_prior_sum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train,Y_train,X_test,Y_test,learning_rate,num_epochs,print_cost,layers_dims):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(1234)\n",
    "    n_x = X_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    n_y = Y_train.shape[0]\n",
    "    costs =[]\n",
    "    (X,Y) = placeholders(n_x,n_y)\n",
    "    param_normal = initialization(layers_dims)\n",
    "    ZL,samples_W,samples_b = forward_propagation(X,layers_dims,param_normal)\n",
    "    loss = compute_cost(layers_dims,samples_W,samples_b,param_normal,ZL,Y_train)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss) \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            _,epoch_cost = sess.run([optimizer,loss],feed_dict={X : X_train, Y : Y_train})\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "        logit_final_temp = tf.nn.softmax(ZL,axis=0)\n",
    "        list_logit_train = []\n",
    "        list_logit_test = []\n",
    "        list_Z = []\n",
    "        for pred in range(0,500):\n",
    "            logit_final_test = logit_final_temp.eval(feed_dict={X: X_test})\n",
    "            logit_final_train = logit_final_temp.eval(feed_dict = {X: X_train})\n",
    "            Z = ZL.eval(feed_dict={X: X_test})\n",
    "            list_logit_train.append(logit_final_train)\n",
    "            list_logit_test.append(logit_final_test)\n",
    "            list_Z.append(Z)\n",
    "            if pred == 0:\n",
    "                arr_pred_train = np.argmax(logit_final_train, axis= 0)\n",
    "                arr_pred_test = np.argmax(logit_final_test,axis=0)\n",
    "                train_accuracy =  np.sum(arr_pred_train == Ytrain) / len(arr_pred_train)\n",
    "                test_accuracy = np.sum(arr_pred_test == Ytest) / len(arr_pred_test)\n",
    "                grad = tf.gradients(logit_final_temp,X)\n",
    "                grad_val = sess.run(grad, feed_dict = {X:X_train})\n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    return [train_accuracy,test_accuracy,list_logit_test,list_logit_train,arr_pred_test,arr_pred_train,grad_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df_pca2,rs):\n",
    "    for l in range(len(list(df_pca2['label'].unique()))):\n",
    "        df_temp = df_pca2.loc[df_pca2['label'] == l]\n",
    "        if l == 0:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(df_temp.iloc[:,:-1],df_temp.iloc[:,-1], test_size=0.20, random_state=rs)\n",
    "        else:\n",
    "            X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(df_temp.iloc[:,:-1],df_temp.iloc[:,-1], test_size=0.20, random_state=rs)\n",
    "            X_train = pd.concat([X_train,X_train_temp])\n",
    "            X_test = pd.concat([X_test,X_test_temp])\n",
    "            y_train = pd.concat([y_train,y_train_temp])\n",
    "            y_test = pd.concat([y_test,y_test_temp])\n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
