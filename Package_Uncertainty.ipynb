{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Users\\Prasoon\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = pd.read_csv('D:/New Folder/labels.csv',header=1)\n",
    "label = list(df_label['Abbreviation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_plot_new(dict_result_train,uncertainty_type,y_test,arr_pred_test,al_test,image_name,cancer_types,scal_fac=1e9):\n",
    "    #fig = plt.figure(figsize = (20,20))\n",
    "    #fig.text(0.5, 0.04, '', ha='center', va='center',fontsize=30)\n",
    "    #if uncertainty_type == 'Aleatoric':\n",
    "        #fig.text(0.06, 0.5, 'Aleatoric Uncertainty', ha='center', va='center', rotation='vertical',fontsize=30)\n",
    "\n",
    "    #if uncertainty_type == 'Epistemic':\n",
    "        #fig.text(0.06, 0.5, 'Epistemic Uncertainty (x '+str(r'$10^{-9}$')+')', ha='center', va='center', rotation='vertical',fontsize=30)\n",
    "\n",
    "    #cancer_types = ['LAML','ACC','BLCA','LGG','BRCA','CESC','CHOL','COAD','UCEC','ESCA','GBM','HNSC','KIRC','KIRP','LIHC','LUAD','LUSC','DLBC','MESO','OV','PAAD','PCPG','PRAD','READ','SKCM','STAD','TGCT','THYM','THCA','UCS','UVM']\n",
    "    dict_mean_train = {}\n",
    "    dict_mean_test_corr = {}\n",
    "    dict_mean_test_incorr = {}\n",
    "    for i in range(len(cancer_types)):\n",
    "    \n",
    "        #test = dict_result_test[i][0]\n",
    "        #pred = dict_result_test[i][1]\n",
    "        #al = dict_result_test[i][2]\n",
    "        #ep = dict_result_test[i][3]\n",
    "        train = dict_result_train[i][0]\n",
    "        pred_train = dict_result_train[i][1]\n",
    "        al_train_temp = dict_result_train[i][2]\n",
    "        ep_train_temp = dict_result_train[i][3]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #train = dict_result_train[i][2]\n",
    "        #a = np.mean(dict_prob_temp[i],axis=0)\n",
    "        #b = np.std(dict_prob_temp[i],axis=0) * 1e3 * 2\n",
    "        #x = np.arange(0,len(test))\n",
    "        #ax = fig.add_subplot(7,5,i+1)\n",
    "        #plt.errorbar(x, a, b, linestyle='None',marker='.',ecolor='r',c='g',markeredgewidth=0.00001)\n",
    "        \n",
    "        list_index = list(np.where(arr_pred_test == i)[0])\n",
    "        list_index2 = list(np.where(y_test==i)[0])\n",
    "    \n",
    "        correct_index = []\n",
    "        incorrect_index = []\n",
    "        for j in range(len(list_index)):\n",
    "            if (list_index[j] in list_index2) == True:\n",
    "                correct_index.append(j)\n",
    "            else:\n",
    "                incorrect_index.append(j)\n",
    "        #print(correct_index)\n",
    "        \n",
    "        corr_al = al_test[list(np.array(list_index)[correct_index])]\n",
    "        incorr_al = al_test[list(np.array(list_index)[incorrect_index])]\n",
    "        \n",
    "        #corr_al+incorr_al\n",
    "        list_combined = list(corr_al)+list(incorr_al)\n",
    "        \n",
    "        index_temp_incorr = []\n",
    "        index_temp = []\n",
    "        for j in range(len(list_combined)):\n",
    "            index_temp.append(j)\n",
    "            if (j >= len(corr_al)):\n",
    "                index_temp_incorr.append(j)\n",
    "        #print(index_temp_incorr)\n",
    "                \n",
    "        #c = zip(list_combined,index_temp)\n",
    "        \n",
    "        l_final, l_index_final = shuffle(list_combined, index_temp, random_state=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #l_final = [e[0] for e in c]\n",
    "        #l_index_final = [e[1] for e in c]\n",
    "        \n",
    "        uncorrect = []\n",
    "        for j in range(len(l_index_final)):\n",
    "            if (l_index_final[j] in index_temp_incorr) ==True:\n",
    "                uncorrect.append(j)\n",
    "        #print(uncorrect)      \n",
    "            \n",
    "        \n",
    "        correct_train = []\n",
    "        uncorrect_train = []\n",
    "        for k in range(len(list(pred_train))):\n",
    "            if (pred_train[k] != train[k]):\n",
    "                uncorrect_train.append(k)\n",
    "            else:\n",
    "                correct_train.append(k)\n",
    "\n",
    "        if uncertainty_type == 'Aleatoric':\n",
    "\n",
    "            corr = np.mean(corr_al)\n",
    "            uncorr = np.mean(incorr_al)\n",
    "            corr_train = np.mean(al_train_temp[correct_train])\n",
    "            #bar_list = ax.bar(np.arange(0,len(l_final)),l_final,width=0.1)\n",
    "            dict_mean_train[i] = corr_train\n",
    "            dict_mean_test_corr[i] = corr\n",
    "            dict_mean_test_incorr[i] = uncorr\n",
    "            \n",
    "        if uncertainty_type == 'Epistemic':\n",
    "            \n",
    "            corr = np.mean(corr_al)*scal_fac\n",
    "            uncorr = np.mean(incorr_al)*scal_fac\n",
    "            corr_train = np.mean(ep_train_temp[correct_train])*scal_fac\n",
    "            #bar_list = ax.bar(np.arange(0,len(l_final)),np.array(l_final)*scal_fac,width=0.1)\n",
    "            dict_mean_train[i] = corr_train\n",
    "            dict_mean_test_corr[i] = corr\n",
    "            dict_mean_test_incorr[i] = uncorr\n",
    "            \n",
    "        #for j in range(len(uncorrect)):\n",
    "            #bar_list[uncorrect[j]].set_color('r')\n",
    "        #ax.axhline(y=corr_train, c = 'g')\n",
    "        #ax.axhline(y=corr, c = 'y')\n",
    "        #ax.axhline(y=uncorr, c = 'm')\n",
    "        #plt.axhline(y=uncorr_epistemic,c= 'm')\n",
    "        #plt.ylabel('Epistemic Uncertainty (x '+str(r'$10^{-9}$')+')',fontsize = 15)\n",
    "        #plt.xlabel('Samples',fontsize=15)\n",
    "        #plt.show()\n",
    "\n",
    "        #ax.set_title(cancer_types[i]+' ('+str(len(l_final))+')')\n",
    "        #ax.set_ylim(0,1.5)\n",
    "        #ax.set_xticklabels('')\n",
    "    #fig.savefig('Desktop/paper_figures/'+str(image_name)+'.pdf', format='pdf', dpi=120,bbox_inches='tight')\n",
    "    return dict_mean_train,dict_mean_test_corr,dict_mean_test_incorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_calculation(logits,y_test,arr_pred_test,type_uncer,t,t_l,image_name,scal_fac=1e-9):\n",
    "    \n",
    "        aleo_list = []\n",
    "        for j in range(len(logits)):\n",
    "            prob_list = []\n",
    "            for i in range(logits[j].shape[1]):\n",
    "                arg = np.argmax(logits[j][:,i])\n",
    "                prob = logits[j][:,i][arg]\n",
    "                prob_list.append(prob)\n",
    "            aleo = list(np.array(prob_list) - np.square(np.array(prob_list)))\n",
    "            aleo_list.append(aleo)\n",
    "            \n",
    "        epi_list = []\n",
    "        for j in range(len(logits)):\n",
    "            prob_list = []\n",
    "            for i in range(logits[j].shape[1]):\n",
    "                arg = np.argmax(logits[j][:,i])\n",
    "                prob = logits[j][:,i][arg]\n",
    "                prob_list.append(prob)\n",
    "            epi_list.append(np.array(prob_list))\n",
    "        epistemic_uncertainty = np.mean(np.square((np.array(epi_list)-np.mean(np.array(epi_list),axis=0))),axis=0)\n",
    "        aleoteric_uncertainty = np.mean(np.array(aleo_list),axis=0)\n",
    "        \n",
    "        uncorrect = []\n",
    "        for i in range(len(list(arr_pred_test))):\n",
    "            if (arr_pred_test[i] != np.array(y_test)[i]):\n",
    "                uncorrect.append(i)\n",
    "                \n",
    "        list_aleoteric_correct = []\n",
    "        list_aleoteric_incorrect = []\n",
    "        for i in range(len(aleoteric_uncertainty)):\n",
    "            if (i in uncorrect) == True:\n",
    "                list_aleoteric_incorrect.append(aleoteric_uncertainty[i])\n",
    "            if (i in uncorrect) == False:\n",
    "                list_aleoteric_correct.append(aleoteric_uncertainty[i])\n",
    "                \n",
    "        list_epistemic_correct = []\n",
    "        list_epistemic_incorrect = []\n",
    "        for i in range(len(epistemic_uncertainty)):\n",
    "            if (i in uncorrect) == True:\n",
    "                list_epistemic_incorrect.append(epistemic_uncertainty[i])\n",
    "            if (i in uncorrect) == False:\n",
    "                list_epistemic_correct.append(epistemic_uncertainty[i])\n",
    "                \n",
    "        corr = np.mean(np.array(list_aleoteric_correct))\n",
    "        uncorr = np.mean(np.array(list_aleoteric_incorrect))\n",
    "        corr_epistemic = np.mean(np.array(list_epistemic_correct))\n",
    "        uncorr_epistemic = np.mean(np.array(list_epistemic_incorrect))\n",
    "        \n",
    "        if type_uncer == 'Aleatoric':\n",
    "            fig = plt.figure()\n",
    "            ax1 = fig.add_axes([0, 0, 1, 1])\n",
    "            ax2 = fig.add_axes()\n",
    "            ax2 = ax1.twinx()\n",
    "            ax1.tick_params(axis='y', labelcolor='b')\n",
    "            ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "            #bar_list = plt.bar(np.arange(0,len(aleoteric_uncertainty)),aleoteric_uncertainty,label='Correct Predictions')\n",
    "            #for i in range(len(uncorrect)):\n",
    "                #if (i == len(uncorrect)-1):\n",
    "                    #bar_list[uncorrect[i]].set_color('r')\n",
    "                    #bar_list[uncorrect[i]].set_label('Incorrect Predictions')\n",
    "                #else:\n",
    "                    #bar_list[uncorrect[i]].set_color('r')\n",
    "            #plt.xticks([])\n",
    "            sns.kdeplot(list_aleoteric_correct,color='b',ax=ax1)\n",
    "            sns.kdeplot(list_aleoteric_incorrect,color='r',ax=ax2).set(xlim=(0, 0.35))\n",
    "   \n",
    "            if t == 'Train':\n",
    "                plt.axvline(x=corr,c= 'b',linestyle='--',label = 'Mean uncertainty (correct)')\n",
    "                plt.axvline(x=uncorr, c = 'r',linestyle='--',label = 'Mean uncertainty (incorrect)')\n",
    "                \n",
    "            #plt.axhline(y=corr, c = 'y')\n",
    "            \n",
    "            if t == 'Validation':\n",
    "                plt.axhline(y=corr,c= 'g',label='Mean uncertainty for correct predictions')\n",
    "                plt.axhline(y=uncorr, c = 'm',label='Mean uncertainty for incorrect predictions')\n",
    "                plt.legend(bbox_to_anchor=(1,1),prop={'size':10})\n",
    "   \n",
    "            if t== 'Test':\n",
    "                plt.axvline(x=corr, c = 'b',linestyle='--',label= 'Mean test uncertainty (correct)')\n",
    "                plt.axvline(x=uncorr,c= 'r',linestyle='--',label = 'Mean test uncertaity (incorrect)')\n",
    "                plt.axvline(x=t_l, c = 'g',linestyle='--',label = 'Mean train uncertainty (correct)')\n",
    "                \n",
    "            plt.legend(bbox_to_anchor=(1.6,1))    \n",
    "            #labs = [l.get_label() for l in leg]\n",
    "            #ax1.legend(leg, labs, loc=(1,1))\n",
    "                \n",
    "            ax1.set_ylabel('Density (Correct Predictions)', color='b',fontsize=15)\n",
    "            ax2.set_ylabel('Density (Incorrect Predictions)', color='r',fontsize=15)\n",
    "\n",
    "            ax1.set_xlabel('Aleatoric Uncertainty',fontsize = 15)\n",
    "            #plt.xlabel('Aleatoric Uncertainty',fontsize = 15)\n",
    "            #plt.ylabel('Samples',fontsize=15)\n",
    "            #fig.savefig('Desktop/paper_revision/'+str(image_name)+'.pdf', format='pdf', dpi=1200,bbox_inches='tight')\n",
    "            plt.show()\n",
    "            return [aleoteric_uncertainty,corr,uncorr]\n",
    "\n",
    "        if type_uncer == 'Epistemic':\n",
    "            fig = plt.figure()\n",
    "            ax1 = fig.add_axes([0, 0, 1, 1])\n",
    "            ax2 = fig.add_axes()\n",
    "            ax2 = ax1.twinx()\n",
    "            ax1.tick_params(axis='y', labelcolor='b')\n",
    "            ax2.tick_params(axis='y', labelcolor='r')\n",
    "            #bar_list = plt.bar(np.arange(0,len(epistemic_uncertainty)),epistemic_uncertainty*scal_fac)\n",
    "            #for i in range(len(uncorrect)):\n",
    "                #bar_list[uncorrect[i]].set_color('r')\n",
    "            #plt.xticks([])\n",
    "            \n",
    "            sns.kdeplot(np.array(list_epistemic_correct)/scal_fac,color='b',ax=ax1)\n",
    "            sns.kdeplot(np.array(list_epistemic_incorrect)/scal_fac,color='r',ax=ax2).set(xlim=(0, 2))\n",
    "   \n",
    "            if t == 'Train':\n",
    "                plt.axvline(x=corr_epistemic/scal_fac, c = 'b',linestyle='--',label = 'Mean uncertainty (correct)')\n",
    "                plt.axvline(x=uncorr_epistemic/scal_fac,c= 'r',linestyle='--',label = 'Mean uncertainty (incorrect)')\n",
    "                #leg = l1+l2\n",
    "                #plt.axhline(y=corr, c = 'y')\n",
    "   \n",
    "            if t== 'Test':\n",
    "                plt.axvline(x=corr_epistemic/scal_fac, c = 'b',linestyle='--',label = 'Mean test uncertainty (correct)')\n",
    "                plt.axvline(x=uncorr_epistemic/scal_fac,c= 'r',linestyle='--',label = 'Mean test uncertainty (incorrect)')\n",
    "                plt.axvline(x=t_l*scal_fac, c = 'g',linestyle='--',label='Mean train uncertainty (correct)')\n",
    "                \n",
    "                \n",
    "            #labs = [l.get_label() for l in leg]\n",
    "            plt.legend(bbox_to_anchor=(1.6,1))\n",
    "            \n",
    "            ax1.set_ylabel('Density (Correct Predictions)', color='b',fontsize=15)\n",
    "            ax2.set_ylabel('Density (Incorrect Predictions)', color='r',fontsize=15)\n",
    "            ax1.set_xlabel('Epistemic Uncertainty (x '+str(r'$10^{'+str(scal_fac).split('e')[-1]+'}$')+')',fontsize = 15)\n",
    "            #plt.axhline(y=corr_epistemic, c = 'g')\n",
    "            #plt.axhline(y=uncorr_epistemic,c= 'm')\n",
    "            #plt.xlabel('Epistemic Uncertainty (x '+str(r'$10^{-9}$')+')',fontsize = 15)\n",
    "            #plt.ylabel('Samples',fontsize=15)\n",
    "            fig.savefig('Desktop/paper_revision/'+str(image_name)+'.pdf', format='pdf', dpi=1200,bbox_inches='tight')\n",
    "            plt.show()\n",
    "            return [epistemic_uncertainty,corr_epistemic,uncorr_epistemic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def com_al_un(logits,arg_passed='argmax'):\n",
    "    aleo_list = []\n",
    "    for j in range(len(logits)):\n",
    "        prob_list = []\n",
    "        for i in range(logits[j].shape[1]):\n",
    "            if arg_passed=='argmax':\n",
    "                arg = np.argmax(logits[j][:,i])\n",
    "            else:\n",
    "                arg = arg_passed   \n",
    "            prob = logits[j][:,i][arg]\n",
    "            prob_list.append(prob)\n",
    "        aleo = list(np.array(prob_list) - np.square(np.array(prob_list)))\n",
    "        aleo_list.append(aleo)\n",
    "        \n",
    "    aleatoric_uncertainty = np.mean(np.array(aleo_list),axis=0)\n",
    "    \n",
    "    return aleatoric_uncertainty\n",
    "\n",
    "def com_ep_un(logits,arg_passed='argmax'):        \n",
    "    epi_list = []\n",
    "    for j in range(len(logits)):\n",
    "        prob_list = []\n",
    "        for i in range(logits[j].shape[1]):\n",
    "            if arg_passed=='argmax':\n",
    "                arg = np.argmax(logits[j][:,i])\n",
    "            else:\n",
    "                arg = arg_passed\n",
    "            prob = logits[j][:,i][arg]\n",
    "            prob_list.append(prob)\n",
    "        epi_list.append(np.array(prob_list))\n",
    "        \n",
    "    epistemic_uncertainty = np.mean(np.square((np.array(epi_list)-np.mean(np.array(epi_list),axis=0))),axis=0)\n",
    "    \n",
    "    return epistemic_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_correction(test_logits):\n",
    "    \n",
    "    adjusted_logits_complete = []\n",
    "    mean_test_logits = sum(test_logits)/len(test_logits)\n",
    "    func_mean_test_logits = np.log(np.array(mean_test_logits)/(1-np.array(mean_test_logits)))\n",
    "    \n",
    "    \n",
    "    for index in range(test_logits[0].shape[0]):\n",
    "    \n",
    "        #epistemic_test = np.sqrt(com_ep_un(test_logits,index))\n",
    "        #epistemic_test_norm = epistemic_test/np.max(epistemic_test)\n",
    "        epistemic_test = np.sqrt(com_ep_un(test_logits,index))\n",
    "        epistemic_test_norm = epistemic_test/np.max(epistemic_test)\n",
    "\n",
    "        X = epistemic_test_norm\n",
    "        Y = list(func_mean_test_logits[index,:])\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        adj_logits = func_mean_test_logits[index,:]-model.params[1]*epistemic_test_norm\n",
    "        func_inv_adj_logits = np.exp(adj_logits)/(1+np.exp(adj_logits))\n",
    "        adjusted_logits_complete.append(func_inv_adj_logits)\n",
    "\n",
    "    return adjusted_logits_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty(test_logits):\n",
    "    \n",
    "    adjusted_logits_complete = []\n",
    "    #mean_test_logits = sum(Z)/len(Z)\n",
    "    #func_mean_test_logits = mean_test_logits\n",
    "    #mean_train_logits = sum(train_logits)/len(logits)\n",
    "    mean_test_logits = sum(test_logits)/len(test_logits)\n",
    "    func_mean_test_logits = np.log(np.array(mean_test_logits)/(1-np.array(mean_test_logits)))\n",
    "    #func_mean_test_logits = mean_test_logits\n",
    "    \n",
    "    #y_train_ols = []\n",
    "    \n",
    "    #for i in range(len(y_train)):\n",
    "        #if np.argmax(mean_train_logits)[:,i] == y_test[i]:\n",
    "           # y_train_ols.append(1)\n",
    "        #else:\n",
    "            #y_train_ols.appendd(0)\n",
    "            \n",
    "    #y_test_ols = []\n",
    "    #for i in range(len(y_test)):\n",
    "        #if np.argmax(mean_test_logits)[:,i] == y_test[i]:\n",
    "            #y_test_ols.append(1)\n",
    "        #else:\n",
    "            #y_test_ols.append(0)\n",
    "    \n",
    "    \n",
    "    for index in range(3):\n",
    "    \n",
    "        #aleatoric_test = np.sqrt(com_al_un(test_logits,index))\n",
    "        #aleatoric_test_norm = aleatoric_test/np.max(aleatoric_test)\n",
    "        epistemic_test = np.sqrt(com_ep_un(test_logits,index))\n",
    "        epistemic_test_norm = epistemic_test/np.max(epistemic_test)\n",
    "\n",
    "        X = epistemic_test_norm\n",
    "        #X = np.vstack((aleatoric_test_norm,epistemic_test_norm)).T\n",
    "        Y = list(func_mean_test_logits[index,:])\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        #adj_logits = func_mean_test_logits[index,:]-model.params[1]*aleatoric_test_norm-model.params[2]*epistemic_test_norm\n",
    "        adj_logits = func_mean_test_logits[index,:]-model.params[1]*epistemic_test_norm\n",
    "        func_inv_adj_logits = np.exp(adj_logits)/(1+np.exp(adj_logits))\n",
    "        #func_inv_adj_logits = adj_logits\n",
    "        adjusted_logits_complete.append(func_inv_adj_logits)\n",
    "\n",
    "    return adjusted_logits_complete,model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def uncertainty_correction(test_logits,y_test):\n",
    "    \n",
    "    adjusted_logits_complete = []\n",
    "    #mean_test_logits = sum(Z)/len(Z)\n",
    "    #func_mean_test_logits = mean_test_logits\n",
    "    #mean_train_logits = sum(train_logits)/len(logits)\n",
    "    mean_test_logits = sum(test_logits)/len(test_logits)\n",
    "    func_mean_test_logits = np.log(np.array(mean_test_logits)/(1-np.array(mean_test_logits)))\n",
    "    #func_mean_test_logits = mean_test_logits\n",
    "    \n",
    "    #y_train_ols = []\n",
    "    \n",
    "    #for i in range(len(y_train)):\n",
    "        #if np.argmax(mean_train_logits)[:,i] == y_test[i]:\n",
    "           # y_train_ols.append(1)\n",
    "        #else:\n",
    "            #y_train_ols.appendd(0)\n",
    "            \n",
    "    #y_test_ols = []\n",
    "    #for i in range(len(y_test)):\n",
    "        #if np.argmax(mean_test_logits)[:,i] == y_test[i]:\n",
    "            #y_test_ols.append(1)\n",
    "        #else:\n",
    "            #y_test_ols.append(0)\n",
    "    \n",
    "    \n",
    "    for index in range(31):\n",
    "    \n",
    "        aleatoric_test = com_al_un(test_logits,index)\n",
    "        aleatoric_test_norm = aleatoric_test/np.max(aleatoric_test)\n",
    "        #epistemic_test = np.sqrt(com_ep_un(test_logits,index))\n",
    "        #epistemic_test_norm = epistemic_test/np.max(epistemic_test)\n",
    "\n",
    "        #X = epistemic_test_norm\n",
    "        X = (aleatoric_test_norm).T\n",
    "        Y = list(func_mean_test_logits[index,:])\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        #adj_logits = func_mean_test_logits[index,:]-model.params[1]*aleatoric_test_norm-model.params[2]*epistemic_test_norm\n",
    "        #adj_logits = func_mean_test_logits[index,:]-model.params[1]*aleatoric_test_norm-model.params[2]*epistemic_test_norm\n",
    "        adj_logits = func_mean_test_logits[index,:]-model.params[1]*aleatoric_test_norm\n",
    "        func_inv_adj_logits = np.exp(adj_logits)/(1+np.exp(adj_logits))\n",
    "        #func_inv_adj_logits = adj_logits\n",
    "        adjusted_logits_complete.append(func_inv_adj_logits)\n",
    "\n",
    "    return adjusted_logits_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def uncertainty(test_logits,list_params='Train'):\n",
    "    epistemic_list =[]\n",
    "    \n",
    "    adjusted_logits_complete = []\n",
    "    #mean_test_logits = sum(Z)/len(Z)\n",
    "    #func_mean_test_logits = mean_test_logits\n",
    "    #mean_train_logits = sum(train_logits)/len(logits)\n",
    "    mean_test_logits = sum(test_logits)/len(test_logits)\n",
    "    func_mean_test_logits = np.log(np.array(mean_test_logits)/(1-np.array(mean_test_logits)))\n",
    "    #func_mean_test_logits = mean_test_logits\n",
    "    \n",
    "    #y_train_ols = []\n",
    "    \n",
    "    #for i in range(len(y_train)):\n",
    "        #if np.argmax(mean_train_logits)[:,i] == y_test[i]:\n",
    "           # y_train_ols.append(1)\n",
    "        #else:\n",
    "            #y_train_ols.appendd(0)\n",
    "            \n",
    "    #y_test_ols = []\n",
    "    #for i in range(len(y_test)):\n",
    "        #if np.argmax(mean_test_logits)[:,i] == y_test[i]:\n",
    "            #y_test_ols.append(1)\n",
    "        #else:\n",
    "            #y_test_ols.append(0)\n",
    "    \n",
    "    \n",
    "    #for index in range(3):\n",
    "    list_model_params = []\n",
    "    \n",
    "    if list_params == 'Train':\n",
    "        \n",
    "        for index in range(test_logits[0].shape[0]):\n",
    "    #aleatoric_test = np.sqrt(com_al_un(test_logits,index))\n",
    "    #aleatoric_test_norm = aleatoric_test/np.max(aleatoric_test)\n",
    "        \n",
    "        #aleatoric_test = np.sqrt(com_al_un(test_logits,index))\n",
    "            epistemic_test = np.sqrt(com_ep_un(test_logits,index))\n",
    "            #max_al = np.max(aleatoric_test)\n",
    "            #max_ep = np.max(epistemic_test)\n",
    "            #aleatoric_test_norm = aleatoric_test/max_al\n",
    "            epistemic_test_norm = epistemic_test\n",
    "\n",
    "            X = epistemic_test_norm\n",
    "            #X = np.vstack((aleatoric_test_norm,epistemic_test_norm)).T\n",
    "            Y = list(func_mean_test_logits[index,:])\n",
    "            X = sm.add_constant(X)\n",
    "            model = sm.OLS(Y, X).fit()\n",
    "            adj_logits = func_mean_test_logits[index,:]-model.params[1]*epistemic_test_norm\n",
    "            list_model_params.append(model.params[1])\n",
    "            #model_params = get_params(epistemic_test_norm,Y)\n",
    "            #adj_logits = func_mean_test_logits[index,:]-epistemic_test_norm*model.params[1]\n",
    "            #func_inv_adj_logits = np.exp(adj_logits)/(1+np.exp(adj_logits))\n",
    "            #func_inv_adj_logits = adj_logits\n",
    "            #adjusted_logits_complete.append(func_inv_adj_logits)\n",
    "\n",
    "        return epistemic_test,list_model_params \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        for index in range(test_logits[0].shape[0]):\n",
    "    \n",
    "        #aleatoric_test = np.sqrt(com_al_un(test_logits,index))\n",
    "        #aleatoric_test_norm = aleatoric_test/np.max(aleatoric_test)\n",
    "            epistemic_test = np.sqrt(com_ep_un(test_logits,index))\n",
    "            #func_inv_adj_logits = adj_logits\n",
    "            epistemic_list.append(list(epistemic_test))\n",
    "\n",
    "\n",
    "\n",
    "        return epistemic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_analysis(logits_train,y_train,arr_pred_train,logits,y_test,arr_pred_test,scal_fac,analysis_type):\n",
    "    [al_train,mu_train_ac,mu_train_nac] = uncertainty_calculation(logits_train,y_train,arr_pred_train,'Aleatoric','Train',_,analysis_type)\n",
    "    [ep_train,mu_train_ec,mu_train_nec] = uncertainty_calculation(logits_train,y_train,arr_pred_train,'Epistemic','Train',_,analysis_type,scal_fac)\n",
    "    [al_test,mu_test_ac,mu_test_nac] = uncertainty_calculation(logits,y_test,arr_pred_test,'Aleatoric','Test',mu_train_ac,analysis_type)\n",
    "    [ep_test,mu_test_ec,mu_test_nec] = uncertainty_calculation(logits,y_test,arr_pred_test,'Epistemic','Test',mu_train_ec,analysis_type,scal_fac)\n",
    "    dict_numtrain_samples = Counter(np.array(y_train))\n",
    "    dict_numtest_samples = Counter(np.array(y_test))\n",
    "    dict_result_train = type_wise_results(dict_numtrain_samples,np.array(y_train),arr_pred_train,al_train,ep_train)\n",
    "    dict_result_test = type_wise_results(dict_numtest_samples,np.array(y_test),arr_pred_test,al_test,ep_test)\n",
    "    dict_result_train = type_wise_results(dict_numtrain_samples,np.array(y_train),arr_pred_train,al_train,ep_train)\n",
    "    dict_result_test = type_wise_results(dict_numtest_samples,np.array(y_test),arr_pred_test,al_test,ep_test)\n",
    "    #dict_mean_ep,dict_mean_test_corr,dict_mean_test_incorr = cancer_wise_uncertainty(dict_result_train,'Epistemic',np.array(y_test),arr_pred_test,ep_test,'epistemic_uncertainty_cancer_wise')\n",
    "    dict_mean_ep,dict_mean_test_corr,dict_mean_test_incorr = detailed_plot_new(dict_result_train,'Epistemic',np.array(y_test),arr_pred_test,ep_test,'epistemic_uncertainty_cancer_wise',cancer_types,1/scal_fac)\n",
    "    list_final_pred = []\n",
    "    list_final_test = []\n",
    "    uncertain = []\n",
    "    certain = []\n",
    "    for i in range(len(arr_pred_test)):\n",
    "        if (ep_test[i]/scal_fac <= dict_mean_ep[arr_pred_test[i]]):\n",
    "            list_final_test.append(np.array(y_test)[i])\n",
    "            list_final_pred.append(arr_pred_test[i])\n",
    "\n",
    "            certain.append(i)\n",
    "\n",
    "        else:\n",
    "            uncertain.append(i)\n",
    "            \n",
    "    start_index = 0\n",
    "    for i in range(logits[0].shape[0]):\n",
    "        print(i)\n",
    "        temp_index = Counter(y_test)[i]\n",
    "        end_index = start_index+temp_index\n",
    "        logits_temp = list(np.array(logits)[:,:,start_index:end_index])\n",
    "        adjusted_logits_temp = uncertainty_correction(logits_temp)\n",
    "        print(np.array(adjusted_logits_temp).shape)\n",
    "        if i == 0:\n",
    "            adjusted_logits_complete = np.array(adjusted_logits_temp)\n",
    "        else:\n",
    "            adjusted_logits_complete = np.concatenate((adjusted_logits_complete,np.array(adjusted_logits_temp)),axis=1)\n",
    "        start_index = end_index\n",
    "    res = adjusted_logits_complete\n",
    "    arr_pred_test_new = []\n",
    "    counter = 0\n",
    "    for i in range(logits[0].shape[1]):\n",
    "        if np.argmax(np.array(res)[:,i]) == list(y_test)[i]:\n",
    "            counter = counter+1\n",
    "        arr_pred_test_new.append(np.argmax(np.array(res)[:,i]))\n",
    "    #counter/len(arr_pred_test_new)\n",
    "    print('EpICC\\n')\n",
    "    print(accuracy_score(y_test,arr_pred_test_new))\n",
    "    print(np.mean(precision_recall_fscore_support(y_test,arr_pred_test_new)[1]))\n",
    "    print('Filtering\\n')\n",
    "    print(accuracy_score(list_final_test,list_final_pred))\n",
    "    print(np.mean(precision_recall_fscore_support(list_final_test,list_final_pred)[1]))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    for key in dict_mean_test_corr.keys():\n",
    "        if analysis_type != 'lgg_subtypes':\n",
    "            plt.bar(key-0.1,dict_mean_ep[key],width=0.05,color='green')\n",
    "            plt.bar(key,dict_mean_test_corr[key],width=0.05,color='blue')\n",
    "            plt.bar(key+0.1,dict_mean_test_incorr[key],width=0.05,color='red')\n",
    "        else:\n",
    "            plt.bar(key-0.2,dict_mean_ep[key],width=0.1,color='green')#,label='Correct train')\n",
    "            plt.bar(key,dict_mean_test_corr[key],width=0.1,color='blue')#,label='Correct test')\n",
    "            plt.bar(key+0.2,dict_mean_test_incorr[key],width=0.1,color='red')#,label = 'Incorrect test')\n",
    "    plt.ylabel('Mean Uncertainty (x'+str(r'$10^{'+str(scal_fac).split('e')[-1]+'}$')+')',fontsize = 15)\n",
    "    plt.xlabel(analysis_type.split('_')[0].upper()+' '+analysis_type.split('_')[1],fontsize=20)\n",
    "    plt.xticks(np.arange(0,len(cancer_types)),cancer_types,fontsize=15)\n",
    "    plt.legend(prop={'size':12})\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.grid(True,which='major')\n",
    "    #ax.yaxis.grid(True,which='minor',linestyle='--')\n",
    "    fig.savefig('Desktop/paper_revision/uncert_comparison_'+analysis_type+'.pdf', format='pdf', dpi=1200,bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return dict_mean_ep,dict_mean_test_corr,dict_mean_test_incorr,res,list_final_test,list_final_pred,arr_pred_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
